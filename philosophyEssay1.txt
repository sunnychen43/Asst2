Haleva 
Ezra Haleva
Professors Michael Glanzberg and Lauren Lyons
Intro to Philosophy Assignment #4
23 November 2020
Utilitarianism and the Morality of a Promise
Humans have a special skill that we perhaps take for granted, a decision making ability. We, in some sense incredibly, actually have some power over our life experience as a result of the decisions we make. By choosing to eat food, we can remove an experience of hunger. By choosing to save a friend, we can ensure the friend’s continuing presence in our life. Whenever faced with a choice, we can predict the outcomes of each option, pick whichever outcome we prefer, and actually get to live in that outcome. With this power, an immediate question follows, “which outcomes do I actually prefer?” The answer to this question defines a system, or philosophy, by which one can determine or guide their decisions. Answering this question, however, is no easy task. Many different philosophies attempt to evaluate outcomes in order to effectively guide our behavior. Utilitarianism is one such philosophy. In exploring the morality of a promise, I will first describe basic utilitarianism concerning simple pain and simple pleasure. Then I’ll analyze what seems to be the intuitive or “right” moral approach to keeping promises, the correct behavioral recommendation that we hope to reproduce with utilitarianism. I’ll then use utilitarianism to produce a behavioral recommendation and compare it to what we would expect, emphasizing certain gaps in the morality of promises provided by simple utilitarianism. I’ll introduce Mill’s extension of utilitarianism to fill those gaps and, finally, assess some of the deeper issues with utilitarianism which seemingly can’t be rectified in any simple or straightforward manner.  
What is utilitarianism? Utilitarianism defines a set of societal preferences by introducing two concepts. First, we define simple pleasure as objectively good. Second, we define pain as objectively bad. Utilitarianism then states a preference for “the greatest amount of pleasure minus pain among all people”. That is, we add up everyone’s pleasure, subtract from that everyone’s pain, and maximize. A fundamental aspect to note is that the recipient of the pain or pleasure is not considered. All the pain could be given to one sorry individual, and all the pleasure to another, and it would make no difference in the behavioral recommendation. As long as there is more pleasure minus pain overall than other options, nothing else matters.


Why do we keep promises? The immediate answer that comes to mind is to maintain some inner value or virtue of integrity as well as building trust. By keeping my word, I communicate to others I am trustworthy, and trust is a future producer of utility. I also accomplish for my own personal satisfaction a virtue of integrity. Whether or not we should then keep a promise usually comes down to a more specific analysis of the proposed violation. Does the benefit of the violation outweigh the loss of breaking the promise? If you had promised not to save a drowning person, we might certainly say the benefit does outweigh the cost. If you promised not to cheat on an exam, it likely doesn’t outweigh the cost. As we will see, utilitarianism is very good at this type of analysis, and then often produces a good recommendation when used to analyze a promise. 


Applying utilitarianism, what does it say about the morality of a promise? Does utilitarianism ever prefer the outcome of a broken promise? It absolutely does. In fact, utilitarianism could even be argued to promote the most vicious and deceitful manipulation, if the circumstances were right. That last stipulation is really the key, “if the circumstances were right”, because utilitarianism doesn’t say anything directly about promises or any virtues like honesty or integrity. Utilitarianism only cares about the amounts of pleasure and pain in the various outcomes, nothing else. It’s then very easy to imagine a scenario where it recommends integrity and another where it recommends deceit. Suppose a business partner makes you promise to be truthful in your business dealings. Breaking that promise might afford some small monetary gain in the short term, but in the long term you would lose out on all the future utility that could be gained from the arrangement. Utilitarianism would then recommend you to keep the promise and maintain the relationship of trust.  Conversely, if you are Jewish in world war 2 Germany and a Gestapo officer makes you promise to tell the truth about your religion, utilitarianism would rightly tell you to lie through your teeth. The pain caused to you by being sent to a concentration camp would far outweigh any pleasure the Gestapo officer might receive in making a successful arrest. 


In these two cases, utilitarianism seems to produce a correct answer, but a simple utilitarianism would still seem to undervalue the intangible, virtuous benefits of keeping a promise, those of integrity and honesty. This undervaluation can easily skew the calculation to be inaccurate. For example, simple utilitarianism might recommend we break a promise so that we can steal a plate of french fries or a bottle of pleasure pills. This is clearly against our intuitive sense of morality, but how can we rectify this?


By applying a further extension of utilitarianism from J. S. Mill, we can expand the initial definitions of pain and pleasure to accommodate the intangible pleasures of virtue. Instead of just simple pleasures like a tasty food, or simple pain like a paper cut, Mill describes a more involved or “higher” pleasure, such as that of music, or the result of a significant life accomplishment. With that is also described a deeper pain, such as the anguish and grief of losing a loved one. To Mill, these varieties of pain and pleasure are far more significant than the simple versions and take priority in all considerations. That is, if we have a choice between even an infinite amount of a simple pleasure and a finite amount of the described “higher” pleasure, the recommendation would be to choose the finite, and yet more valuable, higher pleasure. Applying these definitions, we get an even more accurate recommendation. Instead of breaking a promise for the plate of french fries, you would value your integrity infinitely more than any simple pleasure. Only in the face of an equally virtuous pleasure or to prevent and equally terrible pain would you break a promise.


After Mill’s augmentations, utilitarianism does seem to provide an accurate moral system for behavioral recommendations where a promise is the significant factor. There are, however, still some larger issues with utilitarianism which, though not directly related to a promise, would manifest in any provided recommendation. The fact that there is no distinction between recipients of pain or pleasure creates a glaring and inevitable opportunity for injustice and inequity. Taking a very straightforward example, suppose there are two people, one very rich in utility and one very poor in utility. If we had the option to give 100 utility points to the rich person or 99 to the poor person, our common sense morality would recommend giving 99 to the poor person. Utilitarianism, however, would recommend giving the 100 to the rich person every time. The fact that the rich person is already well off isn’t even accounted for. The fact that the poor person is comparatively suffering is, too, unaccounted for. This leaves a glaring weakness in the utilitarian argument, literally lacking any guarantee of justice whatsoever. Though this may not relate directly to promises, inevitably in the required calculation some similar case will come up, and the resultant recommendation will similarly be unjust.


Perhaps it can be acknowledged that utilitarianism treats promises themselves overall quite accurately, however it can’t escape its fundamental shortcomings. Utilitarianism, by itself, will always have this issue of injustice. This then begs the question, can we take the good treatment of promises and leave behind the issues of inequity and injustice? One possible way to do this might be to attribute some value to equity in the initial calculation, but there is then the issue of how much value to attribute to equity, for which there doesn’t seem to be an intuitive answer. 


Producing a moral system to guide our behavior in a way that resonates with a certain inner morality has been a difficult challenge through time. Many different schools of thought have attempted this, and it would seem by the very fact that this essay is being written that none of them are entirely perfect. Utilitarianism in its careful analysis of each scenario, and especially when intangible pleasures like integrity and honesty are counted, seems to produce a good recommendation with regard to promises. This recommendation, however, is still built upon the foundation of utilitarianism which is fundamentally flawed in its lack of justice and equity. Further, it seems this flaw runs very deep, for there is no straightforward way to rectify it, to take the beneficial analysis of promise and leave behind the deep seated inaccuracies of utilitarianism. For now at least, it seems the utilitarian will still have to rely on the crutch of our moral intuition, even in the favorable scenario of a promise.